---
title: "Confirmatory Factor Analysis"
subtitle: "In Theory"
format: 
  revealjs:
    theme: [default, psych-lecture-theme.scss] 
    css: psych-lecture-style.css
---

## Overview 

```{r}
#| message: false

library(dplyr)
library(tibble)
library(tidyr)
library(readr)
library(lavaan)
library(semPlot)
library(psych)

cfa_data_url <- "https://alopilato88.github.io/psychometrics/materials/data/lecture-8-example-data.csv"

efa_data_url <- "https://alopilato88.github.io/psychometrics/materials/data/lecture-8-example-efa-data.csv"

cfa_data <- readr::read_csv(cfa_data_url)
efa_data <- readr::read_csv(efa_data_url)
```

* Introduction to factor analysis
* Overview of EFA
* Overview of CFA 

## What is Factor Analysis

<strong>Factor analysis</strong> is a data reduction technique that seeks to explain the covariation among a set of observed variables by positing a (usually) smaller subset of unobserved or latent variables. Any covariation that is not explained by the set of latent variables is contained in an error or uniqueness term for each observed variable.

## A Brief History

* Charles Spearman (1904) develops factor analysis to test is theory of general intelligence
* Louis Thurstone (1930s) expands on Spearman's work and shows how to extract multiple factors
* Joreskog (1960s) introduces a way to obtain maximum likelihood solutions for CFA and LISREL is born
* From the 1960s and onward there has been a lot of research on CFA and more general latent variable frameworks 

## The Common Factor Model

```{r}
common_factor_syntax <- "

f1 =~ NA*LV_1_1 + LV_1_2 + LV_1_3 + LV_1_4 + LV_1_5

f1~~1*f1
"

common_factor_fit <- lavaan::cfa(common_factor_syntax, data = efa_data)

semPlot::semPaths(common_factor_fit)
```

## The Statistical Logic Underlying Factor Analysis

* All factor analytic methods (EFA + CFA) try to do one specific thing: <strong>reproduce the observed covariance or correlation structure in one's observed data</strong>. 

* Each factor model you build implies a specific covariance structure, $\mathbf{\Sigma(\Theta)}$, and its your goal to specify a model that implies a covariance structure that closely matches the covariance structure of your observed data, $\mathbf{S}$.

$$min(\mathbf{\Sigma(\Theta)}-\mathbf{S})$$

## Implied Equations for Common Factor Model

$$X_{1}=\lambda_{1}\xi_{1}+\delta_{1}$$

$$X_{2}=\lambda_{2}\xi_{1}+\delta_{2}$$

$$X_{p}=\lambda_{p}\xi_{1}+\delta_{p}$$

## Model Implied Covariance

$$\begin{align}
Cov(X_{p}, X_{p'})&=E[X_{p}X_{p'}]-E[X_{p}]E[X_{p'}]\\
&=E[(\lambda_{p}\xi_{1}+\delta_{p})(\lambda_{p'}\xi_{1}+\delta_{p'})] -E[\lambda_{p}\xi_{1}+\delta_{p}]E[\lambda_{p'}\xi_{1}+\delta_{p'}]\\
&=E[\lambda_{p}\lambda_{p'}\xi_{1}^{2}]-E[\lambda_{p}\xi_{1}]E[\lambda_{p'}\xi_{1}]\\
&=\lambda_{p}\lambda_{p'}(E[\xi_{1}^{2}] - E[\xi_{1}]^{2})\\
&=\lambda_{p}\lambda_{p'}\phi_{1}\\
\end{align}$$

## Implied Equations with Matrix Algebra

$$\begin{bmatrix}
X_{1}\\
X_{2}\\
X_{p}\\
\end{bmatrix}_{p\times1}=
\begin{bmatrix}
\lambda_{1}\\
\lambda_{2}\\
\lambda_{p}
\end{bmatrix}_{p \times q}
\begin{bmatrix}
\xi_{1}\\
\end{bmatrix}_{q \times 1} +
\begin{bmatrix}
\delta_{1}\\
\delta_{2}\\
\delta_{p}\\
\end{bmatrix}_{p \times 1}
$$

$$\mathbf{x}=\mathbf{\Lambda_{x}}\mathbf{\xi}+\mathbf{\delta} $$

## Model Implied Covariance with Matrix Algebra

$$
\begin{align}
E[\mathbf{x}\mathbf{x}']&=E[(\mathbf{\Lambda_{x}}\mathbf{\xi}+\mathbf{\delta})(\mathbf{\Lambda_{x}}\mathbf{\xi}+\mathbf{\delta})^{'}]\\
&=E[\mathbf{\Lambda_{x}}\mathbf{\xi}\mathbf{\xi'}\mathbf{\Lambda_{x}}] + 2E[\mathbf{\Lambda_{x}\xi}\delta']+E[\mathbf{\delta\delta'}]\\
&=\mathbf{\Lambda_{x}}E[\mathbf{\xi\xi'}]\mathbf{\Lambda_{x}} + 2 \mathbf{\Lambda_{x}}E[\mathbf{\xi\delta'}]+E[\mathbf{\delta\delta'}]\\
&=\mathbf{\Lambda_{x}\Phi\Lambda_{x}'}+\mathbf{\Theta_{\delta}}\\
&=\mathbf{\Sigma(\Theta)}\\
\end{align}
$$ 

## Why do we love factor analysis? 

* Specifies latent variables
* Corrects your measurements for (some) measurement error

## Latent variables: What are they?

* Social constructs that exist in the minds of researchers?
* Real entities that exist independent of our measurements?
* A convenient way to summarize emergent phenomena? 
* A variable where there is no "sample realization" for some part of the sample?

## Measurement Error: Where does it go?

* Factor analysis partitions measurement error from observed variance in the theta-delta matrix: $\mathbf{\Theta_{\delta}}$

$$\mathbf{S}=\underbrace{\mathbf{\Lambda_{x}\Phi\Lambda_{x}'}}_{\text{Communality}}+ \underbrace{\mathbf{\Theta_{\delta}}}_{\text{Systematic & Random Error}}$$


## Factor Analysis: Exploratory vs Confirmatory 

* Factor analysis is traditionally separated into two distinct methods: 
  - Exploratory Factor Analysis 
  - Confirmatory Factor Analysis

# Exploratory Factor Analysis

## What is EFA? 

* EFA is a data driven approach to discover the underlying factor structure of one's data based on the covariance structure of one's observed data. 

* Similar to CFA, EFA tries to maximize (or minimize) a fit function that compares the model implied covariance matrix to the observed covariance matrix, but <strong>EFA does not place any constraints on which items measure which factors!</strong>

## Researcher Degrees of Freedom in EFA

1. How to estimate model (Factor Extraction Method)
2. How to determine the number of factors (Factor Retention Method)
3. Determine if factors should correlate with one another (Factor Rotation Method) 

## Factor Extraction in EFA

* Factor extraction in EFA refers to the model estimation method: 
  - Maximum Likelihood (ML) Estimation (My preferred)
  - Principal Axis Factoring (PAF)
  - Variety of least square methods 
  - SO MUCH MORE! 

* Like all things, there is no one size fits all solution, but just <strong>do not use Principal Components Analysis!</strong>

## Factor Retention in EFA

* Given the exploratory nature of EFA, the researcher may not know the exact number of factors underlying their data, so they must rely on several heuristic and statistical methods to determine the number: 
  - Eigenvalues > 1.0 rule (Kaiser Criterion)
  - Scree Plot 
  - Parallel Analysis
  - Very Simple Structure (VSS)
  - Fit Indices 

* Again, there is no one size fits all solution, you should exercise sound judgment, use all of the above methods, and carefully report what you have done. 

## Factor Rotation in EFA

* The TLDR here is that you can specify that the factors in your model should either correlate (oblique rotation) or not correlate (orthogonal rotation). The rotation method you choose should improve the interpretability of your factor model.

* The rule of thumb here is use an oblique rotation like promax or oblimin because all social and behavioral data tends to be correlated.

## EFA Example: R Code

```{r}
#| echo: true
#| eval: false

# Estimate the sample correlation matrix 
efa_cor_matrix <- cor(efa_data)

# Plot the scree plot with the Kaiser Criterion
psych::scree(efa_data, factors = FALSE)

# Determine the % of total variation the components account for
eigen(efa_cor_matrix)$values / sum(eigen(efa_cor_matrix)$values)

# Conduct Parallel Analysis
psych::fa.parallel(
  x = efa_data,
  fa = "pc",
  fm = "ml"
)

# Conduct VSS Analysis
psych::vss(
  x = efa_data,
  n = 8,
  rotate = "promax",
  fm = "ml"
)

# Estimate the EFA model with 3 factors
efa_solution <- 
  psych::fa(
    r = efa_data,
    nfactors = 3,
    fm = "ml"
)
```

## Parallel Analysis

```{r}
efa_cor_matrix <- cor(efa_data)
psych::fa.parallel(
  x = efa_cor_matrix,
  n.obs = 500,
  fa = "pc",
  fm = "ml"
)
```

## VSS {.scrollable}

```{r}
psych::vss(
  x = efa_data,
  n = 8,
  rotate = "promax",
  fm = "ml"
)
```

## EFA Solution {.scrollable}

```{r}
  psych::fa(
    r = efa_data,
    nfactors = 3,
    fm = "ml"
)
```

# Confirmatory Factor Analysis

## What is CFA?

* CFA can be thought of as theory-driven factor analysis:
  - Researcher knows the number of factors (and their interpretation)
  - Researcher knows which indicators load onto which factor 
  - Researcher knows (or has an idea) of the error structure

* With CFA a researcher directly specifies the factor model unlike EFA.

## Our Population Model 

```{r}
pop_mod_syntax <- "

f1 =~ NA*LV_1_1 + LV_1_2 + LV_1_3 + LV_1_4 + LV_1_5
f2 =~ NA*LV_2_1 + LV_2_2 + LV_2_3 + LV_2_4 + LV_2_5
f3 =~ NA*LV_3_1 + LV_3_2 + LV_3_3 + LV_3_4 + LV_3_5

f1~~1*f1
f2~~1*f2
f3~~1*f3
"

population_model <- lavaan::cfa(pop_mod_syntax, data = cfa_data)

semPlot::semPaths(population_model)
```

## Population Equations 

$$
\begin{bmatrix}
X_{11}\\
X_{21}\\
X_{31}\\
\vdots\\
X_{43}\\
X_{53}\\
\end{bmatrix} =
\begin{bmatrix}
\lambda_{11}&0&0\\
\lambda_{21}&0&0\\
\lambda_{31}&0&0\\
\vdots&\vdots&\vdots\\
0&0&\lambda_{43}\\
0&0&\lambda_{53}\\
\end{bmatrix}
\begin{bmatrix}
\xi_{1}\\
\xi_{2}\\
\xi_{3}\\
\end{bmatrix} +
\begin{bmatrix}
\delta_{11}\\
\delta_{21}\\
\delta_{31}\\
\vdots\\
\delta_{43}\\
\delta_{53}\\
\end{bmatrix}
$$

## Model Implied Population Covariance {.smaller}

$$
\mathbf{S} = \mathbf{\Lambda_{x}\Phi\Lambda_{x}'}+\mathbf{\Theta_{\delta}}
$$

$$
\mathbf{\Lambda_{x}\Phi\Lambda_{x}'} = 
\begin{bmatrix}
\lambda_{11}&0&0\\
\lambda_{21}&0&0\\
\lambda_{31}&0&0\\
\vdots&\vdots&\vdots\\
0&0&\lambda_{43}\\
0&0&\lambda_{53}\\
\end{bmatrix}
\begin{bmatrix}
\phi^2_{1}&\phi_{1,2}&\phi_{1,3}\\
\phi_{2,1}&\phi^2_{2}&\phi_{2,3}\\
\phi_{3,1}&\phi_{3,2}&\phi^2_{3}\\
\end{bmatrix}
\begin{bmatrix}
\lambda_{11}&\lambda_{21}&\lambda_{31}&\cdots&0\\
0&0&0&\cdots&0\\
0&0&0&\cdots&\lambda_{53}
\end{bmatrix} 
$$

$$
\mathbf{\Theta_{\delta}} = 
\begin{bmatrix}
\Theta_{1}&0&0&\cdots&0\\
0&\Theta_{2}&0&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&0&\Theta_{15}\\
\end{bmatrix}
$$

## CFA Workflow

![](lecture-8-img/cfa-workflow.png)

## Model Specification 

* Model specification is the process of translating your hypotheses and assumptions into a statistical model (Model building!): 
  - What form the model will take
  - What paths should be present among the different variables 
  - Status of the parameters (fixed, free & constrained, free & unconstrained)

## Form of the Model 

* When building a model, you must first think about what form it will take: linear relationships, variables to include, interactions among variables, etc.

* With CFA, the model form is constrained to linear relationships among the manifest and latent variables and only a measurement model is specified. But there are two specific forms to choose from: 
  - Reflective Model (Most cases will be this!)
  - Formative Model 

## Model Pathways

* With CFA you get to specify:
  - The number and kinds of latent variables (e.g. method factors vs substantive factors)
  - Which variables measure which latent variables (although you typically want to restrict a manifest variable to one latent variable).
  - Which latent variables will correlate with one another.
  - Which disturbance (uniqueness) terms should correlate with one another.

## Parameter Status 

* Each pathway you specify in your model is a parameter that needs to be either estimated by an algorithm or specified by the researcher. 

* Parameters can be: 
  - Fixed by the researcher (set to 0, 1, or any number really)
  - Estimated and Unconstrained (let the model figure it out)
  - Estimated and Constrained (let the model figure it out with a little researcher help)

## The Algebra (ish) Behind Model Identification 

* Find $x$ and $y$:

$$5 = x + y$$

* Find $x$: 

$$5 = 1 + x$$

## Model Identification 

* Identification focuses on understanding what conclusions could be drawn about the population model if one had unlimited data. It is about understanding if their is enough information in the observed data (indicator covariance structure), with an infinite sample size, to draw conclusions about the unobserved population model (statistical parameters).

* Identification + Statistical Inference = Inference 

## Model Identification: Degrees of Freedom {.smaller}

* The amount of information a researcher has available for learning about the population model is equal to the number of unique variances and covariances in one's observed covariance matrix--it has nothing to do with the sample size of one's data: 

$$\frac{p(p+1)}{2}=\text{# of variances and distinct covariances}$$

* The minimal amount of information required for learning about the population model is equal to the number of freely estimated parameters (12 factor loadings, 6 covariances + variances, 15 error variances)

* <strong>Model Degrees of Freedom = Known Information - Number of Estimated Parameters</strong>

## Identifying the Latent Variables

* Because latent variables are not observable in our data, we need to determine the scale (metric) of the latent variable. We can do this in one of two ways: 
  - For each latent variable, fix one factor loadings equal to 1
  - For each latent variable, fix its variance to 1

## Is your model identified?

* Depending on the observed information you have available your model will be:
  - Underidentified (Negative Degrees of Freedom)
  - Just Identified (0 Degrees of Freedom)
  - Overidentified (Greater than 0 Degrees of Freedom)

* You want your model to be overidentified!

## Identification Conditions 

* To ensure your model is identified, you need to have at least 0 degrees of freedom, but it is much better to have more than 0! 

* A sufficient condition for model identification is to have three indicators for each latent variable (but more indicators is (mostly) better!). Think of a unidimensional factor model: how many knowns and unknowns? 

## Empirical Underindentification

* Because identification is not hard enough already, we also need to worry about empirical underidentification, which occurs when the amount of observed information we theoretically have, $\frac{p(p+1)}{2}$, is greater than the information contained in our actual covariance matrix.

* Empirical underidentification occurs for a variety of reasons: 
  - Weak to no correlation (covariation) between variables that have model implied correlations
  - Multicollinearity 

## Model Estimation

* To estimate any statistical model, we need an estimation method that usually tries to maximize or minimize some discrepancy (cost) function. With CFA, we have a handful of estimation methods to choose from:
  - Maximum Likelihood Estimation (Assumes multivariate normality)
  - Robust Maximum Likelihood Estimation (Multivariate normality with corrections)
  - Diagonally Weighted Least-Squares Estimation (Use with categorical data)

## MLE Overview 

* Maximum Likelihood Estimation is a method of estimation that seeks to maximize (usually through iterative algorithms) to maximize the data's likelihood function (or, identically, its log likelihood function). For CFA, MLE assumes the likelihood function is multivariate normal and estimates the parameters by minimizing the following function:

$$F_{ML}=\log|\mathbf{\Sigma(\Theta)}|+\text{tr}[\mathbf{S\Sigma(\Theta)^{-1}}]-\log|\mathbf{S}|-p$$

## Model Evaluation

* Model evaluation asks the question: How well does model reproduce the observed statistical features of my data such as the covariance matrix. 

* Overall goodness of fit
  - Absolute Fit Indices
  - Parsimony Adjusted Fit Indices
  - Incremental Fit Indices
* Areas of localized strain 
* Magnitude & Direction of Parameter Estimates

## Absolute Fit Indices: Chi-Square

* Assuming a lot of things (correct model, multivariate normality, large sample, etc.), $(N-1)F_{ML}$ is distributed $\chi^2_{\text{df}}$.
  - Something to note: $E[\chi^2_{\text{df}}]=\text{df}$

* The chi-square tests the null hypothesis: $\mathbf{S}=\mathbf{\Sigma(\Theta)}$. A significant test indicates that it does not. 

* Although it should always be reported, the $\chi^2$ test has some flaws, most notably it is too sensitive to sample size and at large samples it will almost always be significant. 

## Absolute Fit Indices: SRMR 

* Standardized Root Mean Square Residual or SRMR is equal to the square root of the mean squared difference between the observed correlation matrix, $\mathbf{R}$ and the model implied correlation matrix, $\mathbf{R(\Theta)}$.

* Rule of thumb: SRMR < .08 for a "good" model.

$$
\begin{bmatrix}
1&.4\\
.4&1
\end{bmatrix} - 
\begin{bmatrix}
1&.2\\
.2&1
\end{bmatrix} =
\begin{bmatrix}
0&.2\\
.2&0
\end{bmatrix}, 
\sqrt{\frac{0^2+0^2+.2^2}{3}}
$$

## Parsimony Adjusted Fit Indices: RMSEA

* Root Mean Square Error of Approximation or RMSEA:

$$\sqrt{\frac{\frac{\chi^2-\text{df}}{N-1}}{\text{df}}}$$

* Rule of Thumb: RMSEA < .06 

## Incremental Fit Indices

* Incremental fit indices compare the fit of your model to some "worst fit" comparison model. Unlike the absolute fit indices, values closer to 1 for the incremental fit indices indicate good fit. 
  - Tucker-Lewis Index (TLI or NNFI)
  - Comparative Fit Index (CFI)

## TLI 

$$\frac{\frac{\chi^2_{0}}{df_0}-\frac{\chi^2_{T}}{df_{T}}}{\frac{\chi^2_0}{df_0}-1}$$

* Rule of Thumb: TLI > .95 

## CFI 

$$\frac{\max(\chi^2_{0}-df_{0},0)-\max(\chi^2_{T}-df_{T},0)}{\max(\chi^2_{0}-df_{0},0)}$$

* Rule of Thumb: CFI > .95

## Localized Strain

* The previous indices all deal with global model fit. The indices may indicate your model fits well overall, but could miss areas where your model is performing poorly. 

* Examine the residual correlation matrix and look at modification indices to determine if there are certain parameters that are not being estimated well. 

* Use sound substantive and statistical theory as a guide before making changes!

## Parameter Magnitude & Direction 

* Like all models, you will want to examine each of the parameter estimates and determine if their direction and magnitude corresponds with your hypotheses and theory. 

* If you use some version of ML estimation, you will also have significance tests to guide you.

## Model Respecification 

* After you evaluate your model, you may determine that you need to respecify certain parts of the model:
  - Indicator selection & pattern of indicator-factor relationship
  - Measurement Error Theory (Correlated residuals--be careful here!)
  - Number of factors

## Model Comparison 

* You will always have a "target" theoretical model you are estimating from your data, but you will often also have competing models as well that you would like to test your target model against. To do this, we can rely on: 
  - Chi-square Difference Test for nested models
  - AIC and BIC for non-nested models

## Nested Model Comparison {.smaller}

* A model can be considered nested within another model if its free and unconstrained parameters are a subset of the other model's free and unconstrained parameters. Think of this like hierarchical regression tests. 

* We can use a change-in-chi-square test which is just the restricted models chi-square minus the less restricted model's chi-square. This difference is chi-square distributed with dfs equal to the difference in degrees of freedom.

$$\chi^2_{R}-\chi^2_{U}, \text{df}_{R}-\text{df}_{U}\sim\chi^2_{R-U}$$

* We can test $\chi^2_{R-U}$ using a chi-square distribution with degrees of freedom equal to $\text{df}_{R}-\text{df}_U$. A significant test indicates that the less restricted model fits the data better than the restricted model and we should prefer the less restricted model. When the test is non-significant, we should use the more restricted model under model parsimony. 

## CFA Example with lavaan {.scrollable}

```{r}
#| echo: true
#| eval: false

# Write the model syntax
target_model_syntax <- "
factor_1 =~ LV_1_1 + LV_1_2 + LV_1_3 + LV_1_4 + LV_1_5
factor_2 =~ LV_2_1 + LV_2_2 + LV_2_3 + LV_2_4 + LV_2_5
factor_3 =~ LV_3_1 + LV_3_2 + LV_3_3 + LV_3_4 + LV_3_5

factor_1~~factor_1
factor_2~~factor_2
factor_3~~factor_3
"

# Estimate your model 
target_model_results <- 
  lavaan::cfa(model = target_model_syntax, 
              data = cfa_data,
              estimator = "MLM")

# Evaluate your model fit
summary(target_model_results)
lavaan::fitmeasures(target_model_results)

# Look for localized areas of strain
lavaan::residuals(target_model_results)
lavaan::modificationindices(target_model_results)

# Look at a standardized solution
lavaan::standardizedsolution(target_model_results)
```

## Estimate CFA Model {.scrollable}

```{r}

# Write the model syntax
target_model_syntax <- "
factor_1 =~ LV_1_1 + LV_1_2 + LV_1_3 + LV_1_4 + LV_1_5
factor_2 =~ LV_2_1 + LV_2_2 + LV_2_3 + LV_2_4 + LV_2_5
factor_3 =~ LV_3_1 + LV_3_2 + LV_3_3 + LV_3_4 + LV_3_5

factor_1~~factor_1
factor_2~~factor_2
factor_3~~factor_3
"

# Estimate your model 
target_model_results <- 
  lavaan::cfa(model = target_model_syntax, 
              data = cfa_data,
              estimator = "MLM")

summary(target_model_results, fit.measures = TRUE)
```

## Evaluate Global Fit {.scrollable}

```{r}
# Evaluate your model fit
lavaan::fitmeasures(target_model_results)

```

## Evalute Localized Areas of Strain {.scrollable}

```{r}
# Look for localized areas of strain
lavaan::residuals(target_model_results)
```

## Evalute Localized Areas of Strain {.scrollable}

```{r}
lavaan::modificationindices(target_model_results)
```