---
title: "Introduction to Psychometrics"
subtitle: "Psychometric and Statistical Primer"
format: 
  revealjs:
    theme: [default, psych-lecture-theme.scss] 
    css: psych-lecture-style.css
---

# Welcome!

```{r echo=FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
```

## Introduction

Some things about me:

::: incremental
-   Graduated from this program in 2015 (Triple Dawg!)
-   Currently a manager on McKinsey & Company's internal People, Analytics, & Measurement (PAM) team
-   Love all things analytics!
:::

## Your turn!

::: columns
::: {.column width="60%"}
Round Robin!

-   What's your name?
-   What are you studying?
-   Do you hate, tolerate, or love methods courses?
:::

::: {.column width="40%"}
![](lecture-1-img/round-robin-edit.png)
:::
:::

# What We'll Cover

## Course Goals

-   <strong>Psychometric Theories & Survey Development</strong>
    -   Classical Test Theory, Generalizability Theory, & Validity Theory
-   <strong>Statistical Modeling</strong>
    -   Latent Variable Models, Network Models
-   <strong>R Programming</strong>

## Goals of this Lecture

-   Overview of Psychometrics
    -   The Theoretical: Test Theories & Latent Variable Models
    -   The Practical: Assessment / Questionnaire Design
-   Statistical Primer
    -   Review of Probability Theory
    -   Review of Statistical Inference

## What is Psychometics?

> Psychometrics, or quantitative psychology, is the disciplinary home of a set of statistical models and methods that have been developed primarily to summarize, describe, and draw inferences from empirical data collected in psychological research. 

Jones & Thissen, 2007

# The Theoretical Side

## Theory & Methods in Psychometrics

* <strong>Test Theory</strong>: A mixture of theoretical and statistical models that have been developed to advance educational and psychological measurement.

* <strong>Latent Variable Models</strong>: A family of statistical models that explain statistical dependencies among sets of observed random variables (e.g. survey responses) by appealing to variation in a smaller set of unobserved (latent) variables.

## Test Theories in Psychometrics

* There are three major test theories in Psychometrics:

  - Classical Test Theory (CTT)
  - Generalizability Theory (GT)
  - Item Response Theory (IRT)
  
* The above theories have introduced and further refined the theories of test validity and test reliability.

## Classical Test Theory

- CTT set the foundations for future test theories with the idea that an individual's observed test score is composed of two different additive components: True Score and Error.

- Through its introduction of True Score and Error, CTT helped to formalize the theories of test validity and reliability.

$$\text{Observed Score} = \text{True Score} + \text{Error}$$

## Generalizability Theory

- Built on CTT by further specifying the different sources of error that comprise an individual's observed score.

- GT really furthered the development of reliability theory by introducing different types of reliability indices and drawing a distinction between relative and absolute test score comparisons.

$$\text{Observed Scored} = \mu + E_{rater} + E_{item} + E_{rater\times{}item}$$ 

## Item Response Theory

-   IRT is a modern measurement theory that focuses on the measurement characteristics of the individual items that make up an assessment.

-   Unlike CTT and GT, IRT estimates measurement characteristics of the individual items independently from the abilities of the test takers---this is a big plus!

$$p(x_{ij} = 1 |\theta_{i},b_{j},a_{j})=\frac{1}{1 + e^{-a_{j}(\theta_{i}-b_{j})}}$$

## Latent Variable Models: An Overview {.smaller}

- A family of statistical models that relate covariation in observable variables (e.g. item responses) to latent variables (e.g. psychological constructs).

- Essentially allows one to use observed measurements to infer something about an unobservable construct. 

<table>
  <tr>
    <th></th>
    <th colspan="2"> <center><em>Manifest Variable Distribution</em></center> </th>
  </tr>
  <tr>
    <td> <em>Latent Variable Distribution</em> </td>
    <td>**Continuous**</td>
    <td>**Categorical**</td>
  </tr>
  <tr>
    <td>**Continuous**</td>
    <td>Factor Analysis</td>
    <td>Item Response Theory</td>
  </tr>
  <tr>
    <td>**Categorical**</td>
    <td>Latent Profile Analysis</td>
    <td>Latent Class Analysis</td>
  </tr>
</table>

# The Practical Side

## Some Survey Jargon

![Robinson (2018). Using multi-item psychometric scales for research and practice in human resource management](lecture-1-img/robinson-scale-image.png)

## The Many Uses of Surveys and Assessments

-   Surveys / Assessments are used to assess a wide variety of constructs:
    -   Educational Assessment (SAT, GRE)
    -   Mental health disorders (Beck's Depression Inventory)
    -   Personality (Hogan PEI, Big 5 scales)
    -   Employee Attitude Surveys (There are many!)
    -   Employee Knowledge, Skills, Abilities (KSAs!)

## The Scale Development Process

* **Developing the Scale** 
  - Build a measurement framework
  - Generate items 
  - Evaluate preliminary items
* **Testing the Scale**
  - Pilot the new scale
  - Analyze the pilot data
  - Repeat if needed & possible
* **Put the scale into operation**

# Probability Theory and Statistical Inference

## A Reassuring Reminder

> Statistics is hard, especially when effects are small and variable and measurements are noisy. 

McShane et al., 2019

## Who cares?

-   Why do we care about probability theory and statistical inference? What have they ever done for us?

## Statistical Inference

::: columns
::: {.column width="49%"}

* <strong>Statistical inference</strong> is about drawing conclusions about an entire population based on the information in a sample.

:::
::: {.column width="2%"}
:::

::: {.column width="49%"}

![Sterba, S. K. (2009). Alternative Model-Based and Design-Based Frameworks for Inference From Samples to Populations](lecture-1-img/statistical-inference.png){fig-align="center"}

:::
:::

## Model-Based Statistical Inference

-   Model-based inference allows us to make inferences to an <strong>infinite population</strong> from non-random samples as long as we are willing to make three (strong) assumptions:
    -   Assume a generative statistical model
    -   Assume parametric distributional assumptions on our model
    -   Assume a selection mechanism

## Model-Based Statistical Inference

$$\underbrace{y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}}_{\text{Generative Model}}$$

$$\underbrace{\epsilon_{i}\stackrel{iid}{\sim}N(0, \sigma^{2})}_{\text{Parametric Assumptions & Selection Mech.}}$$

## Role of Probability Theory in Statistics {.small}

::: columns
::: {.column width="35%"}
<br/> <br/> $$y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}$$
:::

::: {.column width="3%"}
<br/> <br/> $$\to$$
:::

::: {.column width="62%"}
$$\underbrace{f_{Y|X}(y_{i}|x_{i})}_{\text{Conditional}\\ \;\;\;\;\text{PDF}}\overbrace{\stackrel{iid}{\sim}}^{\;\;\text{Dist.}\\\text{Assump.}}\underbrace{N(\beta_{0} + \beta_{1}x_{i}, \sigma^{2})}_{\text{Normal PDF}}$$
:::
:::

## What is Probability Theory?

-   <strong>Probability theory</strong> is a mathematical construct used to represent processes involving randomness, unpredictability, or intrinsic uncertainty.

-   <strong>Probability theory is a model!</strong> It allows for us to deal with the uncertainty that comes from being unable to measure every little thing that may be related to our outcome.

## Philsophy of Probability and Inference

<table>
  <tr>
    <th></th>
    <th colspan="2"><em>Philosophy of Probability</em></th>
  </tr>
  <tr>
    <td> <em>Statistical Inference</em> </td>
    <td> <strong>Frequentist</strong> </td>
    <td> <strong>Bayesian</strong> </td>
  </tr>
  <tr>
    <td> <strong>Model Based</strong> </td>
    <td> <center>&#10006;</center></td>
    <td></td>
  </tr>
  <tr>
    <td> <strong>Design Based</strong> </td>
    <td></td>
    <td></td>
  </tr>
</table>

## Random Generative Process (RGP) {.smaller}

::: columns
::: {.column width="50%"}
-   <strong>Random Generative Process</strong> is a mechanism that selects an outcome from a set of multiple outcomes. It consists of three components:
    -   <strong>Sample Space</strong>: Set of all possible states of an RGP.
    -   <strong>Event Space</strong>: Subset of the sample space that consists of possible events that could occur across multiple states.
    -   <strong>Probability Measure</strong>: A function that maps the event to a real number.
:::

::: {.column width="50%"}
![](lecture-1-img/probability-space.png){fig-align="center"}
:::
:::

## Probability Space: Properties {.small}

::: {layout="[[50,50], [50,50]]"}
![$P(A \cup B) = P(A) + P(B)$](lecture-1-img/disjoint-probability-crop.png){fig-align="left" height="200" width="200"}

![$B \subseteq A, P(B) \leq P(A)$](lecture-1-img/subset-probability-crop.png){fig-align="left" height="200" width="200"}

![$P(A \cup B) = P(A) + P(B) - P(A \cap B)$](lecture-1-img/intersect-probability-crop.png){fig-align="left" height="200" width="200"}

![$P(A^{C}) = 1 - P(A)$](lecture-1-img/setminus-probability-crop.png){fig-align="left" height="200" width="200"}
:::

## Joint & Conditional Probability

* <strong>Joint</strong> and <strong>conditional</strong> probability describe how the probabilities of two different events are related.

* <strong>Joint probability</strong> tells us the probability that two events will both occur: 
  + <strong>$P(A \cap B)$</strong>

* <strong>Conditional probability</strong> tell us how the probability of one event changes given that another event already occurred: 
  + $P(A|B) = \frac{P(A \cap B)}{P(B)}$

## Employee Selection Example

For the next few slides, we will imagine we are an workforce analytics analyst on the recruiting team. We want to look at the relationship between a candidate's select assessment performance, `Assessment Outcome`, and the organization's selection decision, `Hiring Outcome`.

## Joint Probabilities: Employee Selection {.smaller}

::: columns
::: {.column width="48%"}
* The table displays the joint probabilities for <strong>Hiring Outcome</strong> and <strong>Assessment Outcome</strong>.
  + What is the probability that <strong>Hiring Outcome = Hire</strong> and <strong>Assessment Outcome = Fail</strong>?
  + What is the probability that <strong>Hiring Outcome = Hire</strong> and <strong>Assessment Outcome = Pass</strong>?
:::

::: {.column width="2%"}
:::

::: {.column width="50%"}
```{r echo=FALSE}

set.seed(9000) # Power level over 9000!
test_group <- c(rep("Fail", 1000), rep("Pass", 4000), rep("Pass+", 500))
test_group_mat <- model.matrix(~as.factor(test_group))
beta <- 
  (c(qlogis(.20), qlogis(.60) - qlogis(.20), qlogis(.80) - qlogis(.20))) |>
  as.matrix(ncol = 1)
hire_latent <- test_group_mat%*%beta 
hire_outcome <- rbinom(length(hire_latent), size = 1, prob = plogis(hire_latent))
hire_data <- 
  data.frame(
    TEST_OUTCOME = test_group,
    HIRE_OUTCOME = hire_outcome,
    HIRE_CATEGORY = dplyr::case_when(
      hire_outcome == 0 ~ "Turndown",
      TRUE ~ "Hire"
    )
  )

xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table() |> 
  round(3) |>
  kableExtra::kbl() |>
  kableExtra::kable_minimal()

```
:::
:::

## Conditonal Probabilities {.smaller}

::: columns
::: {.column width="48%"}

* The tables to the right display the conditional probabilities for <strong>Hiring Outcome</strong> and <strong>Assessment Outcome</strong>.
  + What is the probability that <strong>Hiring Outcome = Hire</strong> given that <strong>Assessment Outcome = Fail</strong>?
  + What is the probability that <strong>Assessment Outcome = Fail</strong> given that <strong>Hiring Category = Hire</strong>?

:::

::: {.column width="2%"}
:::

::: {.column width="50%"}

```{r echo=FALSE}
xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table(1) |> 
  round(3) |>
  kableExtra::kbl(
    caption = "<em><strong>P(Selection Decision) given Assessment Outcome</strong></em>"
  ) |>
  kableExtra::kable_minimal()
```

<br/>

```{r}
xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table(2) |> 
  round(3) |>
  kableExtra::kbl(
    caption = "<em><strong>P(Assessment Outcome) given Selection Decision</strong></em>"
  ) |>
  kableExtra::kable_minimal()
```

:::
:::

## More Conditional Probabilities {.smaller}

::: columns
::: {.column width="33%"}

* <u><strong>Multiplicative Law of Probability</strong></u>

$P(A \cap B)=P(A|B)P(B)$

:::
::: {.column width="33%"}

* <u><strong>Independence</strong></u>

$P(A \cap B) = P(A)P(B)$

<br/>

$\begin{equation}\begin{aligned}
P(A|B) &= \frac{P(A)P(B)}{P(B)} \\
  &= P(A)
\end{aligned}\end{equation}$

:::
::: {.column width="33%"}

* <u><strong>Bayes' Rule</strong></u>

$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$

:::
:::

## Random Variables {.smaller}

::: columns
::: {.column width="45%"}

* <strong>Random variables</strong> are functions that take an event (Hire or Fire) as the input and output a real number (0 or 1). 
* Random variables can be categorized as discrete or continuous based on the range of their output.
  + Discrete Variables have a <strong>countable infinite range</strong> (e.g. integer valued output)
  + Continuous Variables have an <strong>uncountable infinite range</strong> (e.g. real numbers)  

:::

::: {.column width="5%"}
:::

::: {.column width="50%"}

![](lecture-1-img/random-variable-crop.png){fig-align="right"}

:::
:::

## Probability Functions {.smaller}

We can summarize the probabilities of random variables using two related probability functions: the Probability Mass (Density) Function and the Cumulative Distribution Function.  

::: columns
::: {.column width="45%"}

<center><strong><u>Discrete Variables</u></strong></center>
* <strong>Probability Mass Function (PMF):</strong>
  + $f(x)=P(X=x)$

* <strong>Cumulative Distribution Function (CDF)</strong>
  + $F(x)=P(X \leq x)$

:::
::: {.column width="10%"}
:::

::: {.column width="45%"}

<center><strong><u>Continuous Variables</u></strong></center>
* <strong>Probability Density Function (PDF):</strong>
  + $f(x)=\frac{dF(u)}{du}|_{u=x}$

* <strong>Cumulative Distribution Function (CDF):</strong>
  + $F(x)=P(X \leq x) = \int_{-\infty}^{x}f(u)du$
  
:::
:::

## Normal PDF

```{r echo=FALSE}
ggplot2::ggplot(
  NULL,
  ggplot2::aes(
    c(-3, 3)
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(-3, 3)
  ) + 
    ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.line.y = element_line(colour = "#3D3C42"),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Random Variable Value",
    y = "Density"
  )

# ggplot(NULL, aes(c(-3,3))) + 
#   geom_area(stat = "function", fun = dnorm, fill = "red", xlim = c(-3, 0)) +
#   geom_area(stat = "function", fun = dnorm, fill = "blue", xlim = c(0, 3)) +
#   ggplot2::theme(
#     plot.background = element_rect(fill = "#FEFBF6"),
#     panel.background = element_rect(fill = "#FEFBF6"),
#     axis.line.x = element_line(colour = "#3D3C42"),
#     axis.line.y = element_line(colour = "#3D3C42")
#   )
```

## Normal CDF

```{r echo=FALSE}
ggplot2::ggplot(
  NULL,
  ggplot2::aes(
    c(-3, 3)
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = pnorm,
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(-3, 0)
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = pnorm,
    fill = "#FEFBF6",
    color = "#3D3C42",
    xlim = c(0, 3)
  ) +
    ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.line.y = element_line(colour = "#3D3C42"),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Random Variable Value",
    y = "P(X <= 0)"
  )
```

## Bivariate Relationships

* We can use PMFs / PDFs and CDFs to describe the probabilistic relationship between two random variables: 
  + Joint PMF: $f(x, y)=P(X = x, Y = y)$
  + Joint CDF: $F(x,y) = P(X \leq x, Y \leq y)$
  + Marginal PMF: $f_{Y}=P(Y = y) = \sum_{x}f(x,y)$
  + Conditional PMF: $f_{Y|X}(y|x)=\frac{P(Y = y, X = x)}{P(X = x)}=\frac{f(x,y)}{f_{x}(x)}$

## Summarizing Distributions {.smaller}

* We often need to summarize the information contained in marginal and joint distributions. We can do this using several different summary measures: 

  + <strong>Expectation Operator</strong>, $E[X]$, provides us with information about the center of the distribution.
  + <strong>Variance</strong>, $E[(X-E[X])^{2}]$, provides us with information about the shape of the distribution.
  + <strong>Covariance</strong>, $E[(X - E[X])(Y - E[Y])]$, provides us with information about how X and Y move together. 
  + <strong>Conditional Expectation</strong>, $E[Y|X]$, provides us with information about $E[Y]$ given X occurred. 

## Expectation Operator {.smaller}

* The expectation, $E$, of a random variable is the value we would get if we took the mean over an infinite number of realizations of a random variable (if we conducted the same experiment an infinite number of times).

* The expectation operator is a function that takes a random variable as an input and outputs a real number, the expected value of random number: $E[X]=\sum_{x}xf_{x}(x)$.

* Some useful properties:
  + $E[c]=c$ if $c$ is a constant
  + $E[aX]=aE[X]$ if $a$ is a constant 
  + $E[aX + c]=aE[X] + c$---Linearity of Expectations

## Variance & Standard Deviation {.smaller}

* The variance, $\sigma^{2}$, of a random variable tells us about the spread of its distribution. The larger the variance the more unpredictable the random variable. 

* The standard deviation, $\sigma$, is the square root of the variance. It gives us the same information as the variance, but the standard deviation can be interpreted using the scaling of the random variable.

* Some useful properties: 
  + $V[X]=E[X^{2}] - E[X]^{2}$---an easier formula to compute the variance
  + $V[c] = 0$ if c is a constant
  + $V[X + c]= V[X]$ if c is a constant
  + $V[aX]=a^{2}V[X]$ if a is a constant
  + $V[X + Y]=V[X]+V[Y]+2Cov[X, Y]$---variance of a sum score (this will come in handy when we talk about reliability)

## Covariance & Correlation {.smaller}

* The <strong>positive covariance</strong> between two variables, X and Y, tells us if large values of X tend to occur with large values of Y (and small with small). 

* The <strong>negative covariance</strong> tells us if large values of X tend to occur with small values of Y and if small values of X tend to occur with large values of Y. 

* The correlation is just the standardized version of covariance: $\frac{Cov[X, Y]}{\sigma_{x}\sigma_{y}}$

* Some useful properties: 
  + $Cov[X, Y]=E[X, Y] - E[X]E[Y]$---easier to compute 
  + $Cov[X, c]=0$ if c is a constant
  + $Cov[X, Y] = Cov[Y, X]$---covariance is symmetrical 
  + $Cov[X, X]=V[X]$---the covariance of a variable with itself is its variance
  + $Cov[aX + b, cY + d] = acCov[X, Y]$ if a, b, c, and d are constants 

## Conditional Expectation {.smaller}

* The conditional expectation of a random variable, $E[Y|X=x]$, tells us the expected variable of one random variable given the value of another random variable. 

* Conditional expectations allow us to describe how the expected variable of one variable changes once we condition on the observed value of another random variable. This is <strong>exactly</strong> what linear regression does! 

* Some useful properties: 
  + $E[h(x,y)|X=x]=\sum_{y}h(x,y)f_{Y|X}(y|x)$---Conditional expectation of a function
  + $E[g(X)Y|X=x]=g(x)E[Y|X=x]$---Functions of X, $g(X)$, are treated as constants
  + $E[g(X)Y+h(X)|X=x]=g(x)E[Y|X=x]+h(x)$---Linearity of Conditional Expectations

## Samples Instead of Populations {.smaller}

::: columns
::: {.column width="60%"}

* Every statistical model we will use assumes that our sample data are independently and identically distributed---the iid assumption.

* The iid assumption is basically a simplifying assumption that assumes two things: 
  + One observation of a random variable cannot give us any information about another observation of that same random variable (e.g. knowing my score on the selection assessment tells us nothing about Ben's score).
  + Our data all came from the same joint (marginal) probability distribution.

:::
::: {.column width="40%"}

![](lecture-1-img/sampling-cropped.png){fig-align="right"}

:::
:::

## Sample Statistics {.smaller}

* The goal of statistical inference is to estimate population parameters (like the mean and variance) of a population distribution from a sample. Sample statistics allow us to accomplish this goal. 

* <strong>Sample statistics</strong> are functions of our sample data that estimate some feature of a random variable's population distribution: 
  + Sample Mean, $\bar{X}=\frac{\sum{X}}{n}$, estimates the population mean, $\mu=E[X]$
  + Sample Variance, $\hat{\sigma}=\frac{\sum{(X - \bar{X})^2}}{n-1}$, estimates the population variance, $\sigma^2$
  + Estimated Regression Coefficient, $\hat{\beta}$, estimates the population regression coefficient, $\beta$

## Sampling Distributions {.smaller}

* Because a sample statistics is a function of a random variable, it is therefore also a random variable with its own distribution: <strong>Sampling Distribution</strong>.

* Imagine drawing 1,000 <strong>iid</strong> samples of applicant scores on a selection assessment. For each sample you compute the sample mean. You could approximate the sampling distribution of the mean by making a histogram using the 1,000 sample means you computed. 
  + The mean of those 1,000 means would be equal (or close to) the population mean of the selection assessment scores.  
  + The standard deviation of those 1,000 means would equal something called the standard error of the estimate (the standard error of the mean), which tells us how much uncertainty is in our estimate. 


## Sampling Distribution Simulation {.smaller}

::: columns
::: {.column width="40%"}

```{r echo=TRUE}
set.seed(42)
population_mean <- 10
population_variance <- 5
number_samples <- 1000
sample_size <- 50
sample_mean <- numeric()

for(i in 1:number_samples) {
  x <- rnorm(
    n = sample_size, 
    mean = population_mean,
    sd = sqrt(population_variance)
  )
  
  sample_mean_1 <- mean(x)
  sample_mean <- 
    c(sample_mean, 
      sample_mean_1)
}
```

:::
::: {.column width="60%"}

```{r echo=FALSE}
ggplot2::ggplot(
  data = data.frame(X = sample_mean),
  ggplot2::aes(
    x = X
  )
) + 
  ggplot2::geom_histogram(
    binwidth = .10,
    fill = "#A6D1E6",
    color = "#3D3C42"
  ) + 
    ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.line.y = element_line(colour = "#3D3C42"),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Sample Mean",
    y = "Count"
  )
```

```{r echo=TRUE}
mean(sample_mean) # Should be close to the population mean
sd(sample_mean) # Standard Error of the Mean
```
:::
:::
  
## Weak Law of Large Numbers {.smaller}

::: columns
::: {.column width="40%"}

```{r echo=TRUE}
mean_plot_data <- 
  data.frame(
    N = 1:500,
    SAMPLE_MEAN = NA
  )

for(n in 1:500) {
  
  sample_mean <- 
    rnorm(n = n, mean = 5, sd = 1) |>
    mean()
  
  mean_plot_data[n, 2] <- sample_mean
  
  
}
```

:::
::: {.column width="60%"}

```{r echo=FALSE}
ggplot2::ggplot(
  data = mean_plot_data,
  ggplot2::aes(
    x = N, 
    y = SAMPLE_MEAN
  )
) +
  ggplot2::geom_line() + 
    ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.line.y = element_line(colour = "#3D3C42"),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Sample Size",
    y = "Sample Mean"
  ) + 
  ggplot2::geom_hline(
    yintercept = 5,
    color = "#7F5283",
    size = 2
  )
```

:::
:::

## Central Limit Theorem

* <strong>The Central Limit Theorem</strong> proves that the sampling distribution of an estimate created by averaging many ($n \approx 30$) iid random variables will tend to be approximately normal as $n$ increases <strong>even if the underlying distribution of the iid variables is not normal</strong>.  

::: columns
::: {.column width="50%"}

$$Z = \frac{\bar{X}-E[X]}{SE_{\bar{X}}}=\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}$$

:::
::: {.column width="50%"}

$$Z \xrightarrow{d} N(0, 1)$$

:::
:::

## Central Limit Theorem Visualized

```{r}
df <- 
  data.frame(
    FAMILY = rep(c("Exponential", "Poisson", "Normal"), each = 1000),
    N = rep("PDF", 3000),
    OUTCOME = c(rexp(1000, .50), rpois(1000, lambda = 2), rnorm(1000, mean = 2))
  )

for(i in 1:1000) {
  
  # Exponential family
  mean_rexp_30 <- 
    rexp(10, .50) |>
    mean()
  
  mean_rexp_100 <- 
    rexp(100, .50) |>
    mean()
  
  mean_rexp_1000 <- 
    rexp(1000, .50) |>
    mean()
  
  exp_df <- 
    data.frame(
      FAMILY = rep("Exponential", 3),
      N = c("n = 10", "n = 100", "n = 1000"),
      OUTCOME = c(mean_rexp_30, mean_rexp_100, mean_rexp_1000)
    )
  
  # Poisson Family
  mean_pois_30 <-
    rpois(10, lambda = 2) |>
    mean()
  
  mean_pois_100 <-
    rpois(100, lambda = 2) |>
    mean()
  
  mean_pois_1000 <-
    rpois(1000, lambda = 2) |>
    mean()
  
  pois_df <- 
    data.frame(
      FAMILY = rep("Poisson", 3),
      N = c("n = 10", "n = 100", "n = 1000"),
      OUTCOME = c(mean_pois_30, mean_pois_100, mean_pois_1000)
    )
  
  # Normal family
  mean_norm_30 <- 
    rnorm(10, mean = 2) |>
    mean()
  
  mean_norm_100 <-
    rnorm(100, mean = 2) |>
    mean()
  
  mean_norm_1000 <- 
    rnorm(1000, mean = 2) |>
    mean()
  
  norm_df <- 
    data.frame(
      FAMILY = rep("Normal", 3),
      N = c("n = 10", "n = 100", "n = 1000"),
      OUTCOME = c(mean_norm_30, mean_norm_100, mean_norm_1000)
    )
  
  df <- 
    df |>
    rbind(
      exp_df,
      pois_df,
      norm_df
    )
  
}

df <-
  df |>
  dplyr::mutate(
    FAMILY = factor(
      FAMILY, 
      ordered = T, 
      levels = c("Exponential", "Poisson", "Normal")
    ),
    N = factor(
      N,
      ordered = TRUE,
      levels = c("PDF", "n = 10", "n = 100", "n = 1000")
    )
  )


ggplot2::ggplot(
  data = df,
  ggplot2::aes(
    x = OUTCOME
  )
) + 
  ggplot2::facet_grid(
    FAMILY ~ N,
    scales = "free"
  ) +
  ggplot2::geom_histogram(
    fill = "#A6D1E6",
    color = "#3D3C42"
  ) + 
   ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.line.y = element_line(colour = "#3D3C42"),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Sample Means",
    y = "Count"
  )

```

## Statistical Inference: Two Questions 

* Statistical inference asks two broad questions: 
  + What is the <strong>value of an unknown parameter</strong> and how do we <strong>quantify uncertainty</strong> about the estimate? 
  + How consistent are the data with a stated claim or hypothesis about the value of a parameter?


## Population Example 

* We have all the tools we need to start making inferences from the data to the population! 

* <strong>Example</strong>: Here is the population model we will be making inferences about: 

$$Y = \beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\epsilon$$
$$\epsilon \sim N(0, \sigma^{2})$$

* $Y =$ Job Performance, $X_{1} =$ Structured Interview, and $X_{2} =$ Graphology  

## Drawing Inferences from our Data {.smaller}

::: columns
::: {.column width="50%"}

```{r echo=TRUE}
# Set the parameters 
set.seed(32) # Seed for reproducibility
n <- 500 # Sample size
b0 <- 2 # Population Regression Intercept
b1 <- 1 # Population Regression Slope
sigma <- 4 # Population variance

x1 <- sample(
  1:5, 
  size = n, 
  replace = TRUE, 
  prob = c(.10, .15, .15, .30, .30)
)

x2 <- sample(
  1:5,
  size = n,
  replace = TRUE
)

# Generate dependent var.
y <- rnorm(
  n = n, 
  mean = b0 + b1*x1 + 0*x2, 
  sd = sqrt(sigma)
) 

mod <- lm(y ~ x1 + x2) # Estimate model 
```

:::
::: {.column width="50%"}

```{r}
summary(mod)
```

:::
:::

## Conducting Statistical Inference {.smaller}

* How would you use the model output? 

```{r}
summary(mod)
```

## What are P Values? {.smaller}

* The P value is a statistical summary of the compatibility between the observed data and what we would predict or expect to see if we knew the <strong>entire</strong> statistical model were correct. 

$$\text{P value} = P(Data|Model)$$

* Most statistical programming languages report a P value based on the assumption that the effect under investigation is equal to 0. 

* The P value of `r round(summary(mod)$coef[3, 4], 3)` for $X_{2}$ can be interpreted as: "Assuming all of our model assumptions are true including no effect of $X_{2}$, the probability of seeing a value as or more extreme than `r round(summary(mod)$coef[3, 1], 3)` is `r round(summary(mod)$coef[3, 4], 3)`.   

## Computing P Values {.smaller}

* We can compute a P value for any null hypothesis not just a null hypothesis of 0 effect. We just need four things: 
  + Estimated effect 
  + Estimated standard error for the effect
  + Value of the null hypothesis
  + Model degrees of freedom

* Using these four pieces of information, we can construct our test statistic and P value: 

$$T = \frac{Est. - H_{0}}{SE_{est.}}$$
$$\text{P Value}=2\times P(|T| \geq|t| \mid H_{0}=True, Model = True) $$

## Visualizing P Values

```{r}
ggplot2::ggplot(
  NULL,
  ggplot2::aes(
    c(
      qnorm(.0013, sd = summary(mod)$coef[2, 2]),
      qnorm(1 - .0013, sd = summary(mod)$coef[2, 2])
    )
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = summary(mod)$coef[2, 2]),
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(qnorm(.0013, sd = summary(mod)$coef[2, 2]), qnorm(.025, mean = 0, sd = summary(mod)$coef[2, 2]))
  ) +
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = summary(mod)$coef[2, 2]),
    color = "#3D3C42",
    fill = "#FEFBF6",
    xlim = c(qnorm(.025, mean = 0, sd = summary(mod)$coef[2, 2]), qnorm(.975, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = summary(mod)$coef[2, 2]),
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(qnorm(.975, mean = 0, sd = summary(mod)$coef[2, 2]), qnorm(1 - .0013, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", colour = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.ticks.y = ggplot2::element_line(size = 0),
    axis.text.y = ggplot2::element_blank(),
    axis.title.y = ggplot2::element_blank(),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Effect Sizes",
    title = "Null Hypothesis: 0"
  ) +
  ggplot2::geom_vline(
    xintercept = 0,
    color = "#3D3C42",
    size = 2
  ) + 
  ggplot2::geom_vline(
    xintercept = qnorm(1- .0013, sd = summary(mod)$coef[2, 2]),
    color = "#7F5283",
    size = 2
  )

```

## Visualizing P Values

```{r}
ggplot2::ggplot(
  NULL,
  ggplot2::aes(
    c(
      qnorm(.0013, mean = 1, sd = summary(mod)$coef[2, 2]),
      qnorm(1 - .0013, mean = 1, sd = summary(mod)$coef[2, 2])
    )
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 1, sd = summary(mod)$coef[2, 2]),
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(qnorm(.0013, mean = 1, sd = summary(mod)$coef[2, 2]), qnorm(.025, mean = 1, sd = summary(mod)$coef[2, 2]))
  ) +
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 1, sd = summary(mod)$coef[2, 2]),
    color = "#3D3C42",
    fill = "#FEFBF6",
    xlim = c(qnorm(.025, mean = 1, sd = summary(mod)$coef[2, 2]), qnorm(.975, mean = 1, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 1, sd = summary(mod)$coef[2, 2]),
    fill = "#A6D1E6",
    color = "#3D3C42",
    xlim = c(qnorm(.975, mean = 1, sd = summary(mod)$coef[2, 2]), qnorm(1 - .0013, mean = 1, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", color = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.ticks.y = ggplot2::element_line(size = 0),
    axis.text.y = ggplot2::element_blank(),
    axis.title.y = ggplot2::element_blank(),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Effect Sizes",
    title = "Null Hypothesis: 1"
  ) +
  ggplot2::geom_vline(
    xintercept = 1,
    color = "#3D3C42", 
    size = 2
  ) +
  ggplot2::geom_vline(
    xintercept = round(summary(mod)$coef[2, 1], 2),
    color = "#7F5283",
    size = 2
  )

```

## Coding P Values

```{r echo=TRUE}

# X2 coefficient and SE
x2_beta <- summary(mod)$coef[3, 1]
x2_se <- summary(mod)$coef[3, 2]

# Test statistics
test_stat_0 <- (x2_beta - 0) / x2_se # Null = 0
test_stat_25 <- (x2_beta - .25) / x2_se # Null = .25

# P-value for x2 when null hypothesis = 0 (using a central T distribution)
pnorm(abs(test_stat_0), mean = 0, sd = 1, lower.tail = FALSE) * 2 |>
  round(3)

# P-value for x2 when null hypothesis = .25
pnorm(abs(test_stat_25), mean = 0, sd = 1, lower.tail = FALSE) * 2 |>
  round(3)

```


## Confidence Intervals: Two Interpretations {.smaller}

* <strong>Confidence interval</strong>: A range of effect sizes whose tests produced $\text{P Value}\gt .05$ and thus are considered to be more compatible with the data compared to effect sizes outside of the interval <strong>GIVEN</strong> that the model and its assumptions are all correct. 

* <strong>Confidence Interval</strong>: A range of values that contain the population value, estimated by a sample statistic (e.g. regression coefficient), with a given probability---usually 95%. 

$$P(\hat{\beta}-SE_{\hat{\beta}}\times C\leq\beta\leq\hat{\beta}+SE_{\hat{\beta}}\times C)=.95$$

* $C$ is usually set equal to 1.96 to create a normal distribution approximation. 

## Visualizing a Confidence Interval

```{r}
cf_mean <- round(summary(mod)$coef[2, 1], 2)

ggplot2::ggplot(
  NULL,
  ggplot2::aes(
    c(
      qnorm(.0013, mean = cf_mean, sd = summary(mod)$coef[2, 2]),
      qnorm(1 - .0013, mean = cf_mean, sd = summary(mod)$coef[2, 2])
    )
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = cf_mean, sd = summary(mod)$coef[2, 2]),
    fill = "#FEFBF6",
    color = "#3D3C42",
    xlim = c(qnorm(.0013, mean = 1, sd = summary(mod)$coef[2, 2]), qnorm(.025, mean = 1, sd = summary(mod)$coef[2, 2]))
  ) +
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = cf_mean, sd = summary(mod)$coef[2, 2]),
    color = "#3D3C42",
    fill = "#A6D1E6",
    xlim = c(qnorm(.025, mean = cf_mean, sd = summary(mod)$coef[2, 2]), qnorm(.975, mean = cf_mean, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = cf_mean, sd = summary(mod)$coef[2, 2]),
    fill = "#FEFBF6",
    color = "#3D3C42",
    xlim = c(qnorm(.975, mean = cf_mean, sd = summary(mod)$coef[2, 2]), qnorm(1 - .0013, mean = cf_mean, sd = summary(mod)$coef[2, 2]))
  ) + 
  ggplot2::theme(
    plot.background = element_rect(fill = "#FEFBF6", color = "#FEFBF6"),
    panel.background = element_rect(fill = "#FEFBF6"),
    axis.line.x = element_line(colour = "#3D3C42"),
    axis.ticks.y = ggplot2::element_line(size = 0),
    axis.text.y = ggplot2::element_blank(),
    axis.title.y = ggplot2::element_blank(),
    panel.grid.minor = element_line(colour = "#FEFBF6"),
    panel.grid.major = element_line(colour = "#FEFBF6")
  ) +
  ggplot2::labs(
    x = "Effect Sizes",
    title = "95% Confidence Interval"
  ) +
  ggplot2::geom_vline(
    xintercept = round(summary(mod)$coef[2, 1], 2),
    color = "#7F5283",
    size = 2
  )

```

## Confidence Interval: Interpretation 1

```{r echo=TRUE}

confint_mod <- confint(mod) # Creates 95% confidence intervals
x1_coef <- summary(mod)$coef[2, 1]
x1_se <- summary(mod)$coef[2, 2]
x1_lower <- confint_mod[2, 1]
x1_upper <- confint_mod[2, 2]
x1_lower_pvalue <- pnorm(x1_coef, mean = x1_lower, sd = x1_se, lower.tail = F)*2
x1_upper_pvalue <- pnorm(x1_coef, mean = x1_upper, sd = x1_se, lower.tail = T)*2

```

<br>

```{r echo=FALSE}

tibble::tibble(
  X1_COEF = x1_coef,
  X1_STD_ERROR = x1_se,
  X1_LOWER_CI = x1_lower,
  X1_UPPER_CI = x1_upper,
  X1_LOWER_P = x1_lower_pvalue,
  X1_UPPER_P = x1_upper_pvalue
) |>
  round(3)

```

## Confidence Interval: Interpretation 2 {.smaller}

```{r echo=TRUE}

# Set the parameters 
set.seed(3342) # Seed for reproducibility
n <- 500 # Sample size
b0 <- 2 # Population Regression Intercept
b1 <- 1 # Population Regression Slope
sigma <- 4 # Population variance

x1 <- sample(
  1:5, 
  size = n, 
  replace = TRUE, 
  prob = c(.10, .15, .15, .30, .30)
)

x2 <- sample(
  1:5,
  size = n,
  replace = TRUE
)

ci_vec <- numeric() # Create CI vector
for(i in 1:500) {
  
  # Generate dependent var.
  y <- rnorm(
  n = n, 
  mean = b0 + b1*x1 + 0*x2, 
  sd = sqrt(sigma)
) 
  
  mod <- lm(y ~ x1 + x2) # Estimate model 
  
  # Get CI
  ci_mod <- confint(mod)
  ci_lower <- ci_mod[2, 1]
  ci_upper <- ci_mod[2, 2]
  
  # Does CI contain pop. par -- Yes or No
  coverage <- ci_lower <= b1 & b1 <= ci_upper
  ci_vec <- c(ci_vec, coverage)
  
}
```


```{r echo=TRUE}
# What proportion of CIs contained pop. par.?
mean(ci_vec)
```
