---
title: "Psychometrics Overview"
subtitle: "Statistical and Programming Primers"
format: 
  revealjs:
   theme: [default, psych-lecture-theme.scss] 
   css: psych-lecture-style.css
---

```{r}
library(dplyr)
library(knitr)
library(kableExtra)
```

# Welcome!

## Introduction

Some things about me:

::: incremental
-   Graduated from this program in 2015 (Triple Dawg!)
-   Currently a manager on McKinsey & Company's internal People, Analytics, & Measurement (PAM) team
-   Love all things analytics!
:::

## Your turn!

::: columns
::: {.column width="60%"}
Round Robin!

-   What's your name?
-   What are you studying?
-   Do you hate, tolerate, or love methods courses?
:::

::: {.column width="40%"}
![](lecture-1-img/round-robin.png)
:::
:::

# What We'll Cover

## Course Goals

-   <strong>Psychometric Theories & Survey Development</strong>
    -   Classical Test Theory, Generalizability Theory, & Validity Theory
-   <strong>Statistical Modeling</strong>
    -   Latent Variable Models, Network Models
-   <strong>R Programming</strong>

## Goals of this Lecture

-   Overview of Psychometrics
    -   What it is
    -   The theoretical: Test theories & latent variable models
    -   The practical: Assessment / Questionnaire design
-   Statistical Primer
    -   Review of probability theory
    -   Review of statistical inference

## What is Psychometics?

> Psychometrics, or quantitative psychology, is the disciplinary home of a set of statistical models and methods that have been developed primarily to summarize, describe, and draw inferences from empirical data collected in psychological research.

# The Theoretical Side

## Theory & Methods in Psychometrics

-   Test Theory: A mixture of theoretical and statistical models that have been developed to advance educational and psychological measurement.

-   Latent Variable Models: A family of statistical models that explain statistical dependencies among sets of observed random variables (e.g. survey responses) by appealing to variation in a smaller set of unobserved (latent) variables.

## Test Theories in Psychometrics

-   There are three major test theories in Psychometrics:
    -   Classical Test Theory (CTT)
    -   Generalizability Theory (G-Theory)
    -   Item Response Theory (IRT)
-   The above theories have introduced and further refined the theories of test validity and test reliability.

## Classical Test Theory

-   CTT set the foundations for future test theories with the idea that an individual's observed test score is composed of two different additive components: True Score and Error.

$$\text{Observed Score} = \text{True Score} + \text{Error}$$

-   Through its introduction of True Score and Error, CTT helped to formalize the theories of test validity and reliability, respectively.

## Generalizability Theory

-   Build on CTT by further specifying the different sources of error that comprise an individual's observed score.

$$\text{Observed Scored} = \mu + E_{rater} + E_{item} + E_{rater\times{}item}$$ \* G-Theory really furthered the development of reliability theory by introducing different types of reliability indices and drawing a distinction between relative and absolute test score comparisons.

## Item Response Theory

-   IRT is a modern measurement theory that focuses on the measurement characteristics of the individual items that make up an assessment.

-   Unlike CTT and G-Theory, IRT estimates measurement characteristics of the individual items independently from the abilities of the test takers---this is a big plus!

$$p(x_{ij} = 1 |\theta_{i},b_{j},a_{j})=\frac{1}{1 + e^{-a_{j}(\theta_{i}-b_{j})}}$$

## Latent Variable Models: An Overview

# The Practical Side

## Some Survey Jargon

![](lecture-1-img/robinson-scale-image.png)

## The Many Uses of Surveys and Assessments

-   Surveys / Assessments are used to assess a wide variety of constructs:
    -   Knowledge (SAT, GRE)
    -   Mental health disorders (Beck's Depression Inventory)
    -   Personality (Hogan PEI, Big 5 scales)
    -   Employee Attitude Surveys (There are many!)
    -   Skills

## The Scale Development Process

```{mermaid}
flowchart LR

  subgraph cluster_develop[Developing the Scale]
  direction LR
  a1[Measurement <br/> framework]-->a2[Generate <br/> Items]<-->a3[Evaluate <br/> Preliminary Items]
  end

  subgraph cluster_test[Testing the Scale]
  direction LR
  b1[Pilot the <br/> new scale]<-->b2[Analyze the <br/> pilot data]
  b2-->b3[Administer <br/> final scale]-->b4[Analyze the <br/> final data]
  end
  
  cluster_develop-->cluster_test
```

# Probability Theory and Statistical Inference

## Who cares?

-   Why do we care about probability theory and statistical inference? What have they ever done for us?

## Statistical Inference

![](lecture-1-img/statistical-inference.png){fig-align="center"}

## Model-Based Statistical Inference

-   Model-based inference allows us to make inferences to an <strong>infinite population</strong> from non-random samples as long as we are willing to make three (strong) assumptions:
    -   Assume a generative statistical model
    -   Assume parametric distributional assumptions on our model
    -   Assume a selection mechanism

## Model-Based Statistical Inference

$$\underbrace{y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}}_{\text{Generative Model}}$$

$$\underbrace{\epsilon_{i}\stackrel{iid}{\sim}N(0, \sigma^{2})}_{\text{Parametric Assumptions & Selection Mech.}}$$

## Role of Probability Theory in Statistics {.small}

::: columns
::: {.column width="35%"}
<br/> <br/> $$y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}$$
:::

::: {.column width="3%"}
<br/> <br/> $$\to$$
:::

::: {.column width="62%"}
$$\underbrace{f_{Y|X}(y_{i}|x_{i})}_{\text{Conditional}\\ \;\;\;\;\text{PDF}}\overbrace{\stackrel{iid}{\sim}}^{\;\;\text{Dist.}\\\text{Assump.}}\underbrace{N(\beta_{0} + \beta_{1}x_{i}, \sigma^{2})}_{\text{Normal PDF}}$$
:::
:::

## What is Probability Theory?

-   <strong>Probability theory</strong> is a mathematical construct used to represent processes involving randomness, unpredictability, or intrinsic uncertainty.

-   <strong>Probability theory is a model!</strong> It allows for us to deal with the uncertainty that comes from being unable to measure every little thing that may be related to our outcome.

## Random Generative Process (RGP) {.smaller}

::: columns
::: {.column width="50%"}
-   <strong>Random Generative Process</strong> is a mechanism that selects an outcome from a set of multiple outcomes. It consists of three components:
    -   <strong>Sample Space</strong>: Set of all possible states of an RGP.
    -   <strong>Event Space</strong>: Subset of the sample space that consists of possible events that could occur across multiple states.
    -   <strong>Probability Measure</strong>: A function that maps the event to a real number.
:::

::: {.column width="50%"}
![](lecture-1-img/probability-space.png){fig-align="center"}
:::
:::

## Probability Space: Properties {.small}

::: {layout="[[50,50], [50,50]]"}
![$P(A \cup B) = P(A) + P(B)$](lecture-1-img/disjoint-probability-crop.png){fig-align="left" height="200" width="200"}

![$B \subseteq A, P(B) \leq P(A)$](lecture-1-img/subset-probability-crop.png){fig-align="left" height="200" width="200"}

![$P(A \cup B) = P(A) + P(B) - P(A \cap B)$](lecture-1-img/intersect-probability-crop.png){fig-align="left" height="200" width="200"}

![$P(A^{C}) = 1 - P(A)$](lecture-1-img/setminus-probability-crop.png){fig-align="left" height="200" width="200"}
:::

## Joint & Conditional Probability

* <strong>Joint</strong> and <strong>conditional</strong> probability describe how the probabilities of two different events are related.

* <strong>Joint probability</strong> tells us the probability that two events will both occur: 
  + <strong>$P(A \cap B)$</strong>

* <strong>Conditional probability</strong> tell us how the probability of one event changes given that another event already occurred: 
  + $P(A|B) = \frac{P(A \cap B)}{P(B)}$

## Employee Selection Example

## Joint Probabilities: Employee Selection {.smaller}

::: columns
::: {.column width="48%"}
* The table displays the joint probabilities for <strong>Hiring Outcome</strong> and <strong>Assessment Outcome</strong>.
  + What is the probability that <strong>Hiring Outcome = Hire</strong> and <strong>Assessment Outcome = Fail</strong>?
  + What is the probability that <strong>Hiring Outcome = Hire</strong> and <strong>Assessment Outcome = Pass</strong>?
:::

::: {.column width="2%"}
:::

::: {.column width="50%"}
```{r echo=FALSE}

set.seed(9000) # Power level over 9000!
test_group <- c(rep("Fail", 1000), rep("Pass", 4000), rep("Pass+", 500))
test_group_mat <- model.matrix(~as.factor(test_group))
beta <- 
  (c(qlogis(.20), qlogis(.60) - qlogis(.20), qlogis(.80) - qlogis(.20))) |>
  as.matrix(ncol = 1)
hire_latent <- test_group_mat%*%beta 
hire_outcome <- rbinom(length(hire_latent), size = 1, prob = plogis(hire_latent))
hire_data <- 
  data.frame(
    TEST_OUTCOME = test_group,
    HIRE_OUTCOME = hire_outcome,
    HIRE_CATEGORY = dplyr::case_when(
      hire_outcome == 0 ~ "Turndown",
      TRUE ~ "Hire"
    )
  )

xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table() |> 
  round(3) |>
  kableExtra::kbl() |>
  kableExtra::kable_minimal()

```
:::
:::

## Conditonal Probabilities {.smaller}

::: columns
::: {.column width="48%"}

* The tables to the right display the conditional probabilities for <strong>Hiring Outcome</strong> and <strong>Assessment Outcome</strong>.
  + What is the probability that <strong>Hiring Outcome = Hire</strong> given that <strong>Assessment Outcome = Fail</strong>?
  + What is the probability that <strong>Assessment Outcome = Fail</strong> given that <strong>Hiring Category = Hire</strong>?

:::

::: {.column width="2%"}
:::

::: {.column width="50%"}

```{r echo=FALSE}
xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table(1) |> 
  round(3) |>
  kableExtra::kbl(
    caption = "<em><strong>P(Selection Decision) given Assessment Outcome</strong></em>"
  ) |>
  kableExtra::kable_minimal()
```

<br/>

```{r}
xtabs(~TEST_OUTCOME + HIRE_CATEGORY, hire_data) |> 
  prop.table(2) |> 
  round(3) |>
  kableExtra::kbl(
    caption = "<em><strong>P(Assessment Outcome) given Selection Decision</strong></em>"
  ) |>
  kableExtra::kable_minimal()
```

:::
:::

## More Conditional Probabilities {.smaller}

::: columns
::: {.column width="33%"}

* <u><strong>Multiplicative Law of Probability</strong></u>

$P(A \cap B)=P(A|B)P(B)$

:::
::: {.column width="33%"}

* <u><strong>Independence</strong></u>

$P(A \cap B) = P(A)P(B)$

<br/>

$\begin{equation}\begin{aligned}
P(A|B) &= \frac{P(A)P(B)}{P(B)} \\
  &= P(A)
\end{aligned}\end{equation}$

:::
::: {.column width="33%"}

* <u><strong>Bayes' Rule</strong></u>

$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$

:::
:::

## Random Variables {.smaller}

::: columns
::: {.column width="45%"}

* <strong>Random variables</strong> are functions that take an <em>event (Hire or Fire) as the input</em> and <em>output a real number</em> (0 or 1). 
* Random variables can be categorized as discrete or continuous based on the range of their output.
  + Discrete Variables have a <strong>countable infinite range</strong> (e.g. integer valued output)
  + Continuous Variables have an <strong>uncountable infinite range</strong> (e.g. real numbers)  

:::

::: {.column width="5%"}
:::

::: {.column width="50%"}

![](lecture-1-img/random-variable-crop.png){fig-align="right"}

:::
:::

## Probability Functions {.smaller}

We can summarize the probabilities of random variables using two related probability functions: 

::: columns
::: {.column width="45%"}

<center><strong><u>Discrete Variables</u></strong></center>
* <strong>Probability Mass Function (PMF):</strong>
  + $f(x)=P(X=x)$

* <strong>Cumulative Distribution Function (CDF)</strong>
  + $F(x)=P(X \leq x)$

:::
::: {.column width="10%"}
:::

::: {.column width="45%"}

<center><strong><u>Continuous Variables</u></strong></center>
* <strong>Probability Density Function (PDF):</strong>
  + $f(x)=\frac{dF(u)}{du}|_{u=x}$

* <strong>Cumulative Distribution Function (CDF):</strong>
  + $F(x)=P(X \leq x) = \int_{-\infty}^{x}f(u)du$
  
:::
:::

## Normal PDF and CDF

::: columns
::: {.column width="45%"}

```{r echo=FALSE}
ggplot2::ggplot(
  data = data.frame(x = -3:3),
  ggplot2::aes(x = x)
) + 
  ggplot2::xlim(c(-3, 3)) + 
  ggplot2::geom_function(
    fun = dnorm
  ) + 
  ggplot2::theme_minimal()
```

:::

::: {.column width="10%"}
:::

::: {.column width="45%"}

```{r echo=FALSE}
ggplot2::ggplot(
  data = data.frame(x = -3:3),
  ggplot2::aes(x = x)
) + 
  ggplot2::xlim(c(-3, 3)) + 
  ggplot2::geom_function(
    fun = pnorm
  ) + 
  ggplot2::theme_minimal()
```

:::
:::

## Conditional PMF

## Expected Value

## Variance

## Covariance

## Correlation